{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring many different models and short-listing the best ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to proceed to use our training data and our training labels without the variables `city`,`Latitude`,`Longitude`,`change_hunits` `change_hunits`,`studio_1000_1499`, `studio_1500_more`,\n",
    "       `studio_750_999`, `onebed_1000_1499`, `onebed_1500_more`,\n",
    "       `onebed_750_999`, `twobed_1000_1499`, `twobed_1500_more`,\n",
    "       `twobed_750_999`, `threebed_1000_1499`, `threebed_1500_more`,\n",
    "       `threebed_750_999`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next its recommended to try out some models from various categories of Machine Learning algorithms **Note best practice recommends that we cannot simply standardize all of the data once at the beginning and run cross validation on the standardized data. To do so would be allowing the model to peek at the validation set during training.** And so when measuring the performance its is best to standardize our data within each fold in the N-fold cross-validation being done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "np.random.seed(22)\n",
    "#load data\n",
    "def load_housing_data(housing_path):\n",
    "    csv_path = os.path.join(housing_path,'datasets','california_housing.csv')\n",
    "    return pd.read_csv(csv_path,index_col=0)  \n",
    "housing = load_housing_data('C:\\\\Users\\\\Crist\\\\Towncharts\\\\California_Housing_Project\\\\')\n",
    "housing = housing.rename(columns = {'percent_of_rent_to_total':'rent_home_percent'})\n",
    "\n",
    "\n",
    "#make train and test set\n",
    "housing = housing[pd.notnull(housing['med_rental_rate'])]\n",
    "housing = housing.reset_index(drop=True)\n",
    "\n",
    "housing['property_cost_cat'] = pd.cut(housing['med_homeval'],\n",
    "                                  bins = [0,237150,389600,620350,np.inf],\n",
    "                                  labels = [1,2,3,4])\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=22)\n",
    "for train_index,test_index in split.split(housing,housing['property_cost_cat']):\n",
    "    strat_train_set  = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "    \n",
    "#Drop the property value category attribute now that its been used to create the test set and no longer is needed\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"property_cost_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a list of different models in order to shortlist the top promising models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinary Least Squares Linear Regression 5 fold Cross-Validation Results: \n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "Scores [[228.70962322 176.52633545 224.31023843 193.71690671 239.92708296]]\n",
      "Mean 212.6380373537479\n",
      "Standard Deviation 23.66977971984869\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
      "      random_state=None, solver='cholesky', tol=0.001)\n",
      "Ridge Regression 5 fold Cross-Validation Results: \n",
      "Scores [[228.41757545 177.22545992 220.96739757 192.37498221 235.90847068]]\n",
      "Mean 210.97877716511599\n",
      "Standard Deviation 22.408754778383287\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Lasso Regression 5 fold Cross-Validation Results: \n",
      "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=22,\n",
      "      selection='cyclic', tol=0.09, warm_start=False)\n",
      "Scores [[239.38792648 195.17038822 220.81849914 193.15404639 259.1010623 ]]\n",
      "Mean 221.5263845060566\n",
      "Standard Deviation 25.420544076574924\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Elastic Net 5 fold Cross-Validation Results: \n",
      "ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=22, selection='cyclic', tol=0.01, warm_start=False)\n",
      "Scores [[228.91154503 182.52314844 213.02067397 193.27925272 220.2842728 ]]\n",
      "Mean 207.60377858959833\n",
      "Standard Deviation 17.195282199301207\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "RF-Regressor 5 fold Cross-Validation Results: \n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                      n_jobs=None, oob_score=False, random_state=22, verbose=0,\n",
      "                      warm_start=False)\n",
      "Scores [[223.63810401 208.34103968 217.19849917 248.28014952 250.14372876]]\n",
      "Mean 229.5203042292232\n",
      "Standard Deviation 16.80622409083839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "print('Ordinary Least Squares Linear Regression 5 fold Cross-Validation Results: ')\n",
    "print(lin_reg)\n",
    "model_pipeline(lin_reg,strat_train_set,5)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Ridge Regression\n",
    "ridge_reg = Ridge(alpha =1 , solver = 'cholesky')\n",
    "print(ridge_reg)\n",
    "print('Ridge Regression 5 fold Cross-Validation Results: ')\n",
    "model_pipeline(ridge_reg,strat_train_set,5)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Lasso Regression\n",
    "lasso_reg = linear_model.Lasso(alpha=0.1,tol = 0.09,random_state= 22)\n",
    "print('Lasso Regression 5 fold Cross-Validation Results: ')\n",
    "print(lasso_reg)\n",
    "model_pipeline(lasso_reg,strat_train_set,5)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Elastic Net\n",
    "elastic_reg = ElasticNet(alpha = 0.1,tol=0.01, l1_ratio = 0.5,random_state= 22)\n",
    "print('Elastic Net 5 fold Cross-Validation Results: ')\n",
    "print(elastic_reg)\n",
    "model_pipeline(elastic_reg,strat_train_set,5)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Random Forest Regressor\n",
    "forest_reg = RandomForestRegressor(n_estimators=100,random_state= 22)\n",
    "print('RF-Regressor 5 fold Cross-Validation Results: ')\n",
    "print(forest_reg)\n",
    "model_pipeline(forest_reg,strat_train_set,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net is our winner at this point in time with  a mean RMSE of ~207 and std ~17 at this point the model has not achieved our goal of providing an estimate of the median rental rate that is less than 192 dollars. Lets us see if we can improve upon this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helpful thing to do here is to check the noise of the data and how much variance in median rental price our best model can explain currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance with an Elastic Net regresion model\n",
      "ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=22, selection='cyclic', tol=0.01, warm_start=False)\n",
      "Scores [[0.83867501 0.88496919 0.8248445  0.78324435 0.74007055]]\n",
      "Mean 0.8143607210858583\n",
      "Standard Deviation 0.04934350514441448\n"
     ]
    }
   ],
   "source": [
    "#explained variance\n",
    "print('Explained variance with an Elastic Net regresion model')\n",
    "print(elastic_reg)\n",
    "model_pipeline(elastic_reg,strat_train_set,5,checknoise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet accounts for ~ 81% of the variation in our given training data set with a sd of 4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate the relative importance of each attribute \n",
    "for making accurate prediction of median rental rate for feature selection purposes to see if reducing the noise i.e. dropping uninformative attributes improves our predictions. We will use the Random Forest Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.modeling import prepdf_or_featureselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X_train = (prepdf_or_featureselection(strat_train_set,prep=True))['mydata']\n",
    "rf_reg = RandomForestRegressor(n_estimators=100,random_state=22)\n",
    "rf_reg.fit(my_X_train,strat_train_set['med_rental_rate'].copy())\n",
    "feature_importances = rf_reg.feature_importances_\n",
    "\n",
    "#sorted(zip(feature_importances,(prepdf_or_fullpipeline(strat_train_set,onlyprep = True))['mydatacolumns']),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = (prepdf_or_featureselection(strat_train_set,prep = True))['mydatacolumns']\n",
    "importance_results = sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4887781138536719, 'med_owner_cost')\n",
      "(0.21675649535699323, 'med_hcost_ownmortg')\n",
      "(0.050201590357428647, 'med_real_estate_taxes')\n",
      "(0.03587588836602332, 'owned_homes')\n",
      "(0.0347595014957407, 'median_num_ofrooms')\n",
      "(0.0324637039464469, 'med_homeval')\n",
      "(0.025615844525265767, 'rent_home_percent')\n",
      "(0.013820970048462571, 'housing_density')\n",
      "(0.013256862434699219, 'med_hcost_own_wo_mortg')\n",
      "(0.010148870136003196, 'household_size_for_renters')\n",
      "(0.009420439127731117, 'med_year_renter_moved_in')\n",
      "(0.008661245901148296, 'median_year_house_built')\n",
      "(0.00801330904523532, 'med_hval_aspercentof_medearn')\n",
      "(0.007343128332141725, 'area_total_sq_mi')\n",
      "(0.007072207368963794, 'hcost_aspercentof_hincome_ownmortg')\n",
      "(0.007069719982840823, 'med_year_moved_in_for_owners')\n",
      "(0.005911235719694841, 'population')\n",
      "(0.004938470868514768, 'housing_units')\n",
      "(0.004910073403746763, 'household_size_of_howners')\n",
      "(0.004566983165365608, 'med_own_cost_aspercentof_income')\n",
      "(0.00409571060983884, 'family_members_per_hunit')\n",
      "(0.0030364234055547264, 'hcost_as_perc_of_hincome_womortg')\n",
      "(0.0018569607973755054, 'cluster2')\n",
      "(0.000860147955821826, 'cluster1')\n",
      "(0.0005661037952903825, 'cluster0')\n",
      "Total attributes:  25\n"
     ]
    }
   ],
   "source": [
    "for i in importance_results:\n",
    "    print(i)\n",
    "print('Total attributes: ',len(importance_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-86b995833366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimportance_df\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance_results\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'relative_importance'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'value'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relative_importance'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportance_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attribute'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Relative Importance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "importance_df= pd.DataFrame(importance_results,columns = ['relative_importance','value'])\n",
    "plt.figure(figsize = (10,5))\n",
    "chart = sns.barplot(x = 'value',y = 'relative_importance',data = importance_df)\n",
    "plt.xlabel('Attribute')\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.xticks(rotation = 45,horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Owner cost and median housing cost for those who own a mortgage are our 2 most informative attributes in predicting the median rental rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you wanted to look at how attributes for a city contribute to our machine learning estimators model predictions for a given city. In this case the SHAP library along with our random forest model can be used to explain the estimators predictions for San Diego and Los Angeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_df = (strat_train_set.copy()).reset_index(drop=True)\n",
    "imputer = IterativeImputer(max_iter = 10 ,random_state =22,min_value=0)\n",
    "housing_shap = strat_train_set.drop(['city','Latitude','Longitude','change_hunits','studio_1000_1499', 'studio_1500_more',\n",
    "       'studio_750_999', 'onebed_1000_1499', 'onebed_1500_more',\n",
    "       'onebed_750_999', 'twobed_1000_1499', 'twobed_1500_more',\n",
    "       'twobed_750_999', 'threebed_1000_1499', 'threebed_1500_more',\n",
    "       'threebed_750_999','med_rental_rate'],axis=1)\n",
    "\n",
    "imputed_shap = imputer.fit_transform(housing_shap)\n",
    "housing_shap = pd.DataFrame(imputed_shap,columns = housing_shap.columns)\n",
    "housing_labels_shap = np.array(strat_train_set['med_rental_rate'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_diego = city_df.loc[city_df.city=='SanDiego'].index[0]\n",
    "los_angeles = city_df.loc[city_df.city=='LosAngeles'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_rf = RandomForestRegressor(n_estimators=100,random_state= 22)\n",
    "shap_rf.fit(housing_shap,housing_labels_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(shap_rf)\n",
    "shap_values = explainer.shap_values(housing_shap)\n",
    "\n",
    "shap.force_plot(explainer.expected_value,shap_values[san_diego,:],housing_shap.iloc[san_diego,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For San Diego the above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual value for San Diego is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.loc[city_df.city == 'SanDiego','med_rental_rate'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is underpredicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the same for Los Angeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value,shap_values[los_angeles,:],housing_shap.iloc[los_angeles,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual value for Los Angeles is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_df.loc[city_df.city == 'LosAngeles','med_rental_rate'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coloring by Median Home Evaluation highlights that med_own_cost_aspercentof_income (median owner cost as percent of household income) had less impact on median rental rate for areas with a low median home evaluation even as it increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot('med_own_cost_aspercentof_income',shap_values,housing_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the information given to us by the random forest feature importance let us now reuse our `preparedf_or_featureselection` function to explore how many features GridSearchCV recommends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_results = (prepdf_or_featureselection(strat_train_set,myfeature_importances=feature_importances,prep=False))['myfeature_selection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection_results['feature_selection__k']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['med_owner_cost',\n",
       " 'med_hcost_ownmortg',\n",
       " 'med_real_estate_taxes',\n",
       " 'owned_homes',\n",
       " 'median_num_ofrooms',\n",
       " 'med_homeval',\n",
       " 'rent_home_percent',\n",
       " 'housing_density',\n",
       " 'med_hcost_own_wo_mortg',\n",
       " 'household_size_for_renters',\n",
       " 'med_year_renter_moved_in',\n",
       " 'median_year_house_built',\n",
       " 'med_hval_aspercentof_medearn',\n",
       " 'area_total_sq_mi',\n",
       " 'hcost_aspercentof_hincome_ownmortg',\n",
       " 'med_year_moved_in_for_owners',\n",
       " 'population',\n",
       " 'housing_units',\n",
       " 'household_size_of_howners',\n",
       " 'med_own_cost_aspercentof_income',\n",
       " 'family_members_per_hunit',\n",
       " 'hcost_as_perc_of_hincome_womortg',\n",
       " 'cluster2',\n",
       " 'cluster1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informative_features = [i[1] for i in sorted(zip(feature_importances, attributes), reverse=True)][0:feature_selection_results['feature_selection__k']]\n",
    "informative_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently almost all features are useful except one (cluster 0). Lets recheck the models we tried out above now with our 24 features and see by having less noise i.e. reducing the number of irrelevant features the models is trying to fit we can reduce our MSE and see if there is a new winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lin_reg = LinearRegression()\n",
    "print('Ordinary Least Squares Linear Regression 5 fold Cross-Validation Results: ')\n",
    "print(lin_reg)\n",
    "model_pipeline(lin_reg,strat_train_set,5,feature_selection_done= True,myfeatures = informative_features)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Ridge Regression\n",
    "ridge_reg = Ridge(alpha =1 , solver = 'cholesky')\n",
    "print(ridge_reg)\n",
    "print('Ridge Regression 5 fold Cross-Validation Results: ')\n",
    "model_pipeline(ridge_reg,strat_train_set,5,feature_selection_done= True,myfeatures = informative_features)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Lasso Regression\n",
    "lasso_reg = linear_model.Lasso(alpha=0.1,tol = 0.09,random_state= 22)\n",
    "print('Lasso Regression 5 fold Cross-Validation Results: ')\n",
    "print(lasso_reg)\n",
    "model_pipeline(lasso_reg,strat_train_set,5,feature_selection_done= True,myfeatures = informative_features)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Elastic Net\n",
    "elastic_reg = ElasticNet(alpha = 0.1,tol = 0.01, l1_ratio = 0.5,random_state= 22)\n",
    "print('Elastic Net 5 fold Cross-Validation Results: ')\n",
    "print(elastic_reg)\n",
    "model_pipeline(elastic_reg,strat_train_set,5,feature_selection_done= True,myfeatures = informative_features)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "#Random Forest Regressor\n",
    "forest_reg = RandomForestRegressor(n_estimators=100,random_state= 22)\n",
    "print('RF-Regressor 5 fold Cross-Validation Results: ')\n",
    "print(forest_reg)\n",
    "model_pipeline(forest_reg,strat_train_set,5,feature_selection_done= True,myfeatures = informative_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected reducing the number of features down by just one does not improve our RMSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning models and combine them into a great solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the hyper-parameters of the best estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the effects of the parameters of ElasticNet this [link](https://stats.stackexchange.com/questions/84012/choosing-optimal-alpha-in-elastic-net-logistic-regression) was very helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "prep_pipeline = Pipeline([\n",
    "    ('imputer',IterativeImputer(max_iter = 10,random_state = 22,min_value=0)),\n",
    "            ('rob_scaler',RobustScaler())])\n",
    "\n",
    "X_Prepared = prep_pipeline.fit_transform(strat_train_set.loc[:,informative_features].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "elastic_grid = {'alpha':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'l1_ratio':np.arange(0.0, 1.0, 0.1),\n",
    "               'tol':np.arange(0.01, .10, 0.01)}\n",
    "eNet = ElasticNet(random_state=22)\n",
    "elastic_grid = GridSearchCV(eNet,elastic_grid,cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True,n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "elastic_grid.fit(X_Prepared,strat_train_set['med_rental_rate'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-(elastic_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross validation mean for the MSE score is actually higher lets see why that might be with a learning curve.Also note that the complete training data is scaled before cross-validation rather than scaled for each fold during cross validation which might affect our above grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes,train_scores,test_scores = learning_curve(\n",
    "Ridge(alpha =1 , solver = 'cholesky'), X_Prepared,strat_train_set['med_rental_rate'],\n",
    "    train_sizes=[50,100,150,200,288],cv=5,scoring='neg_mean_squared_error',random_state=22\n",
    ")\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score(MSE)\")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MSE might converge to a certain limit even as we gather more training data \n",
    "this could mean that in order to reduce our MSE we are likely going to need to \n",
    "gather further variables of importance in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets check if ensemble methods or boosting might give some improvement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A voting Regressor Ensemble(ElasticNet with Random Forest) 5 fold Cross-Validation Results: \n",
      "Scores [[216.19043793 181.9209085  203.31474648 214.55382382 227.5918772 ]]\n",
      "Mean 208.71435878501106\n",
      "Standard Deviation 15.449249845405017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "elastic_reg = ElasticNet(alpha = 0.1, tol = .01,l1_ratio = 0.5,random_state= 22)\n",
    "forest_reg = RandomForestRegressor(n_estimators=100,random_state= 22)\n",
    "voting_reg = VotingRegressor(\n",
    "            estimators = [('en',elastic_reg),('rf',forest_reg)],\n",
    "            n_jobs = -1)\n",
    "#Random Forest Regressor\n",
    "\n",
    "print('A voting Regressor Ensemble(ElasticNet with Random Forest) 5 fold Cross-Validation Results: ')\n",
    "\n",
    "model_pipeline(voting_reg,strat_train_set,5,feature_selection_done= True,myfeatures = informative_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ensemble of our Elastic Net and our Random Forest model increased our RMSE slightly from ~207 to ~208 but reduces our std from ~17 to ~15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val = train_test_split(\n",
    "                            X_Prepared,strat_train_set['med_rental_rate'],\n",
    "                            test_size = 0.2,random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120,random_state=22)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "         for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators,random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A Gradient Boosting Regressor 5 fold Cross-Validation Results: ')\n",
    "model_pipeline(gbrt_best,strat_train_set,5,feature_selection_done= True,myfeatures = informative_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The std for RMSE for a GradientBoostingRegressor is our lowest yet ~11 but it does have a higher mean RMSE from our lowest of 223 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The System that we will evaluate on the Test Set will be the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making alist of the numerical attributes\n",
    "num_attrs = informative_features.copy()\n",
    "del num_attrs[22:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import joblib\n",
    "# Save the model as a pickle in a file \n",
    "#joblib.dump(winning_pipeline_results['myfinalmodel'], 'trained_final_model.pkl') \n",
    "#joblib.dump(winning_pipeline_results['final_predictions'], 'trained_final_model_predictions.pkl')\n",
    "joblib.dump(strat_train_set['med_rental_rate'], 'training_data_labels.pkl') \n",
    "joblib.dump(strat_test_set['med_rental_rate'].copy(), 'test_data_labels.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final Root Mean Squared Error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "winning_pipeline_results = winning_pipeline(strat_train_set,strat_test_set,voting_reg,feature_selection_done=True,myfeatures=informative_features,numerical_attributes=num_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winning_pipeline_results['final_rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our explained variance in median rental rate by our final model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_pipeline_results['final_expvar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How precise is this estimate lets compute a 95% confidence interval for the generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "confidence =.95\n",
    "squared_errors = (winning_pipeline_results['final_predictions']-strat_test_set['med_rental_rate'])**2\n",
    "np.sqrt(stats.t.interval(confidence,len(squared_errors) - 1,\n",
    "                        loc = squared_errors.mean(),\n",
    "                        scale = stats.sem(squared_errors)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future research perhaps some categorical variables should be considered after seeking domain expertise about valuable categorical variables to consider in the prediction of median rental price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model may not have produced the desired solution to our business problem however in this project we learned quite a lot. In the story to follow we will summarize the\n",
    "key findings of this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"story\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A story about applicable results from the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob Ross is a 40 year old college professor. He has 1 kid in college and is widowed. Unfortunately for Bob he has had  a series of unexpected financial expenses that have rendered him to temporarily fall behind on his mortgage payments on his 3 bedroom house in [San Luis Obispo](https://www.google.com/search?q=san+luis+obispo&oq=sa&aqs=chrome.0.69i59j69i57j69i59l2j69i60l3j69i65.906j0j7&sourceid=chrome&ie=UTF-8), CA. While talking to his friend Tim about his troubles Tim tells him that he can flip the script by renting his home for a period of time. He explains that this may be a good option when two factors are present: Your home would rent for at or more than your mortgage payment and you were able to find an affordable place to stay. San Luis Obispo is a College Town and Bob believes he can rent his home during the school year while he rents a small apartment to help cover the costs before moving back in. Tim says that to determine the rental price of your property, consult directly with a real estate agent or property management company to take a look at comparable rentals in your area. But before doing so Tim invites Bob over to do some research of their own.Tim is savvy with python and has done some research that he presents to Bob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He explains that across California the the top 3 predictors of median rental rates are monthly cost of housing for property owners including mortgage payment, taxes, insurance,and utilities, median housing cost for homeowners with a mortgage(including the cost of the mortgage or other debt), and the median real estate taxes paid by owners of homes in the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Feature_importance_image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally Tim also shows Bob the figure below and suggest that in San Luis Obispo the  median real estate taxes, median home evaluation, and median housing cost for those who own a mortgage contribute to increasing median rental rate. On the other hand it suggest that in San Luis Obispo median owner cost and average number of rooms for housing units in the area and the fact the percent of all occupied housing units that are owned housing units is 38(%)  contribute to decreasing the median rental rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "san_luis_obispo = city_df.loc[city_df.city=='SanLuisObispo'].index[0]\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value,shap_values[san_luis_obispo,:],housing_shap.iloc[san_luis_obispo,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tim tells Bob that he can come up with a prediction \n",
    "of the rental price of his property within a margin of error of 196 dollars that takes into account several pieces of information about his property and the local real estate market.  Bob thanks Tim for his help he tells him that this will give him a better understanding of whether the quote he receives from the consultant is an accurate estimate. Bob then goes to Linda a real estate agent and rather than be in a state of unknowing he knows that the estimate Linda provides to him aligns with the prediction Tim had provided him. As a result Bob has piece of mind that he has received a fair estimate of the rental price of his property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "[Hands-On Machine Learning with Scikit-Learn & TensorFlow](http://shop.oreilly.com/product/0636920142874.do)\n",
    "https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#three\n",
    "    https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf\n",
    "https://elitedatascience.com/primer\n",
    "https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n",
    "\"Towncharts.com - United States Demographics Data.\" United States Demographics data. N.p., 15 Dec. 2016. Web. 04 Sep. 2019. <http://www.towncharts.com/>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
